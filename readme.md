# Тестовое задание Skanestas

# Запуск

`feed` - `python3 feed.py`, 
`web` - `docker compose up`

## Общее описание

Приложение состоит из двух частей:
1) `feed` - отвечает за создание потока данных
2) `web` - предоставляет веб-интерфейс для отрисовки потока в реальном времени

`feed` генерирует поток данных раз в секунду, как указано в задании, складирует в хранилище и отправляет
событие с теми же данными в pubsub шину.

`web` при открытии главной страницы запрашивает в хранилище данные за последнюю минуту и открывает 
websocket подключение, через которое получает новые данные в реальном времени от `feed`

`pubsub` - клиент Pubsub шины, бэкенд - redis.
`storage` - клиент БД, бэкенд - clickhouse.
Клиенты используются в обеих частях, в "боевом" окружении это могли бы быть общие библиотеки.

## Хранилище

Для хранения данных выбрана колоночная БД Clickhouse. Данные каждого ticker хранятся в отдельном столбце,
плюс столбец с таймштампом данных, индексирован. (Таблица имеет 101 столбец).

Такой подход позволяет очень быстро выполнять запросы к конкретным тикерам - сначала БД выполняет поиск по
сортированному столбцу таймштампов (O(log2 n)), а выборка конкретного столбца сводится к вычитыванию соответствующего
файла - каждый столбец clickhouse хранит в отдельном файле. Таким образом, время на обработку незапрашиваемых
столбцов не тратится.

Такой подход хоть и представляется наиболее быстрым, но имеет свои недостатки - при добавлении или удалении
источников данных потребуется смена схемы БД, что приведет хоть и к кратковременному, но даунтайму, что, в зависимости
от бизнес требований, может быть недопустимо.

Для более удобной работы можно рассмотреть вариант с тремя столбцами - время, имя тикера и значение, с индексами
на имени тикера (отсекаем сразу 99% данных) и времени. Время поиска немного увеличится, но обеспечить бесперебойность
работы при добавлении/удалении источников становится легче. Также, в таком варианте проще обрабатывать вариант, когда
данные по каждому из тикеров приходят не одновременно.

## Вебсокет

Для стриминга данных клиентам приложение хранит в памяти список подключений и имена тикеров, чтобы отправлять клиентам
только запрашиваемые ими данные. 

Есть потенциал для улучшения производительности - сейчас используется только
один канал для передачи сообщений, и "расфасовкой" данных занимается бизнес логика веб приложения. Кажется, если
использовать отдельный канал на каждый тикер, и динамически подписываться на запрашиваемые топики, то можно
избавить веб-бэкенд от ненужной работы. Насколько велик будет эффект оценить затрудняюсь.


## Фронтэнд
Отрисовка на uPlot. Есть место для улучшения - между получением данных по GET и началом работы websocket потока
можно пропустить пару событий. Как вариант - отправлять первым сообщением вебсокета немного истории и аккуратно
склеивать. Или запрашивать историю GETот уже после получения первого события, ориентируясь на его таймштамп,
но может увеличить время до первой отрисовки графика.
